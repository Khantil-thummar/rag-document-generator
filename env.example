# =============================================================================
# RAG Document Generator - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp env.example .env
# =============================================================================

# -----------------------------------------------------------------------------
# REQUIRED
# -----------------------------------------------------------------------------

# Your OpenAI API key (get it from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-openai-api-key-here

# -----------------------------------------------------------------------------
# OPTIONAL - OpenAI Models
# -----------------------------------------------------------------------------

# Embedding model for vector generation
# Options: text-embedding-3-small (cheaper), text-embedding-3-large (better quality)
# Default: text-embedding-3-small
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# LLM model for content generation
# Options: gpt-4o-mini (fast, cheap), gpt-4o (better quality, expensive)
# Default: gpt-4o-mini
OPENAI_LLM_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# OPTIONAL - Qdrant Vector Database
# -----------------------------------------------------------------------------

# Directory path for persistent vector storage
# Default: ./qdrant_data
QDRANT_PATH=./qdrant_data

# Collection name in Qdrant
# Default: documents
QDRANT_COLLECTION_NAME=documents

# -----------------------------------------------------------------------------
# OPTIONAL - Document Chunking
# -----------------------------------------------------------------------------

# Maximum tokens per chunk (larger = more context, less precise)
# Default: 500
CHUNK_SIZE=500

# Overlap tokens between chunks (preserves context across boundaries)
# Default: 50
CHUNK_OVERLAP=50

# -----------------------------------------------------------------------------
# OPTIONAL - Retrieval Settings
# -----------------------------------------------------------------------------

# Number of chunks to retrieve for generation
# Higher = more context but slower, Lower = faster but less context
# Default: 5
TOP_K=5

# Minimum similarity score (0.0-1.0) for chunks to be included
# Higher = stricter matching, Lower = more permissive
# Default: 0.3
SIMILARITY_THRESHOLD=0.3

# -----------------------------------------------------------------------------
# OPTIONAL - LLM Generation Settings
# -----------------------------------------------------------------------------

# Maximum tokens for context in the LLM prompt
# Default: 4000
MAX_CONTEXT_TOKENS=4000

# LLM temperature (0.0-1.0): Lower = factual, Higher = creative
# Keep low for document generation
# Default: 0.3
TEMPERATURE=0.3

